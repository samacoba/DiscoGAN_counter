{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import cuda, Variable\n",
    "from chainer import optimizers\n",
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(chainer.Chain):\n",
    "    def __init__(self, ):\n",
    "        super(Generator, self).__init__(\n",
    "            conv1=L.Convolution2D(None, 64, 4, 2, 1),\n",
    "            conv2=L.Convolution2D(None, 128, 4, 2, 1),\n",
    "            norm2=L.BatchNormalization(128),\n",
    "            conv3=L.Convolution2D(None, 256, 4, 2, 1),\n",
    "            norm3=L.BatchNormalization(256),\n",
    "            conv4=L.Convolution2D(None, 512, 4, 2, 1),\n",
    "            norm4=L.BatchNormalization(512),\n",
    "\n",
    "            deconv1=L.Deconvolution2D(None, 256, 4, 2, 1),\n",
    "            dnorm1=L.BatchNormalization(256),\n",
    "            deconv2=L.Deconvolution2D(None, 128, 4, 2, 1),\n",
    "            dnorm2=L.BatchNormalization(128),\n",
    "            deconv3=L.Deconvolution2D(None, 64, 4, 2, 1),\n",
    "            dnorm3=L.BatchNormalization(64),\n",
    "            deconv4=L.Deconvolution2D(None, 3, 4, 2, 1),\n",
    "            )\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        # convolution\n",
    "        h1 = F.leaky_relu(self.conv1(x))\n",
    "        h2 = F.leaky_relu(self.norm2(self.conv2(h1), test=test))\n",
    "        h3 = F.leaky_relu(self.norm3(self.conv3(h2), test=test))\n",
    "        h4 = F.leaky_relu(self.norm4(self.conv4(h3), test=test))\n",
    "\n",
    "        # deconvolution\n",
    "        dh1 = F.leaky_relu(self.dnorm1(self.deconv1(h4), test=test))\n",
    "        dh2 = F.leaky_relu(self.dnorm2(self.deconv2(dh1), test=test))\n",
    "        dh3 = F.leaky_relu(self.dnorm3(self.deconv3(dh2), test=test))\n",
    "        #y = F.tanh(self.deconv4(dh3))\n",
    "        y = self.deconv4(dh3)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Discriminator(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__(\n",
    "            conv1=L.Convolution2D(None, 512, 4, 2, 1),\n",
    "            conv2=L.Convolution2D(None, 256, 4, 2, 1),\n",
    "            norm2=L.BatchNormalization(256),\n",
    "            conv3=L.Convolution2D(None, 128, 4, 2, 1),\n",
    "            norm3=L.BatchNormalization(128),\n",
    "            conv4=L.Convolution2D(None, 64, 4, 2, 1),\n",
    "            norm4=L.BatchNormalization(64),\n",
    "            conv5=L.Convolution2D(None, 1, 4)\n",
    "            )\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        # convolution\n",
    "        h1 = F.leaky_relu(self.conv1(x))\n",
    "        h2 = F.leaky_relu(self.norm2(self.conv2(h1), test=test))\n",
    "        h3 = F.leaky_relu(self.norm3(self.conv3(h2), test=test))\n",
    "        h4 = F.leaky_relu(self.norm4(self.conv4(h3), test=test))\n",
    "\n",
    "        # full connect\n",
    "        # the size of feature map is 4x4.\n",
    "        # So convolution with 4x4 filter is similar to full connect.\n",
    "        y = self.conv5(h4)\n",
    "        return y, [h2, h3, h4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DiscoGANUpdater():\n",
    "    def __init__(self, DataA, DataB, opt_g_ab, opt_g_ba, opt_d_a, opt_d_b):\n",
    "        self.generator_ab = opt_g_ab.target\n",
    "        self.generator_ba = opt_g_ba.target\n",
    "        self.discriminator_a = opt_d_a.target\n",
    "        self.discriminator_b = opt_d_b.target\n",
    "        self._optimizers = {'generator_ab': opt_g_ab,\n",
    "                            'generator_ba': opt_g_ba,\n",
    "                            'discriminator_a': opt_d_a,\n",
    "                            'discriminator_b': opt_d_b}\n",
    "        self.iteration = 0\n",
    "        self.xp = self.generator_ab.xp\n",
    "        self.DataA = DataA\n",
    "        self.DataB = DataB\n",
    "\n",
    "    def compute_loss_gan(self, y_real, y_fake):\n",
    "        batchsize = y_real.shape[0]\n",
    "        loss_dis = 0.5 * F.sum(F.softplus(-y_real) + F.softplus(y_fake))\n",
    "        loss_gen = F.sum(F.softplus(-y_fake))\n",
    "        return loss_dis / batchsize, loss_gen / batchsize\n",
    "\n",
    "    def compute_loss_feat(self, feats_real, feats_fake):\n",
    "        losses = 0\n",
    "        for feat_real, feat_fake in zip(feats_real, feats_fake):\n",
    "            feat_real_mean = F.sum(feat_real, 0) / feat_real.shape[0]\n",
    "            feat_fake_mean = F.sum(feat_fake, 0) / feat_fake.shape[0]\n",
    "            l2 = (feat_real_mean - feat_fake_mean) ** 2\n",
    "            loss = F.sum(l2) / l2.size\n",
    "            losses += loss\n",
    "        return losses\n",
    "\n",
    "    def update_core(self, batchSize = 100):      \n",
    "          \n",
    "        # read data\n",
    "        DataN = data.get_data_N_rand(self.DataA, N_pic = batchSize, imgH = 64, imgW = 64)        \n",
    "        x_a = Variable(self.xp.asarray(DataN['x']))\n",
    "  \n",
    "        DataN = data.get_data_N_rand(self.DataB, N_pic = batchSize, imgH = 64, imgW = 64)        \n",
    "        x_b = Variable(self.xp.asarray(DataN['x']))\n",
    "        \n",
    "        batchsize = x_a.shape[0]\n",
    "\n",
    "        # conversion\n",
    "        x_ab = self.generator_ab(x_a)\n",
    "        x_ba = self.generator_ba(x_b)\n",
    "\n",
    "        # reconversion\n",
    "        x_aba = self.generator_ba(x_ab)\n",
    "        x_bab = self.generator_ab(x_ba)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss_a = F.mean_squared_error(x_a, x_aba)\n",
    "        recon_loss_b = F.mean_squared_error(x_b, x_bab)\n",
    "\n",
    "        # discriminate\n",
    "        y_a_real, feats_a_real = self.discriminator_a(x_a)\n",
    "        y_a_fake, feats_a_fake = self.discriminator_a(x_ba)\n",
    "\n",
    "        y_b_real, feats_b_real = self.discriminator_b(x_b)\n",
    "        y_b_fake, feats_b_fake = self.discriminator_b(x_ab)\n",
    "\n",
    "        # GAN loss\n",
    "        gan_loss_dis_a, gan_loss_gen_a = self.compute_loss_gan(y_a_real, y_a_fake)\n",
    "        feat_loss_a = self.compute_loss_feat(feats_a_real, feats_a_fake)\n",
    "\n",
    "        gan_loss_dis_b, gan_loss_gen_b = self.compute_loss_gan(y_b_real, y_b_fake)\n",
    "        feat_loss_b = self.compute_loss_feat(feats_b_real, feats_b_fake)\n",
    "\n",
    "        # compute loss\n",
    "        if self.iteration < 10000:\n",
    "            rate = 0.01\n",
    "        else:\n",
    "            rate = 0.5\n",
    "\n",
    "        total_loss_gen_a = (1.-rate)*(0.1*gan_loss_gen_b + 0.9*feat_loss_b) + rate * recon_loss_a\n",
    "        total_loss_gen_b = (1.-rate)*(0.1*gan_loss_gen_a + 0.9*feat_loss_a) + rate * recon_loss_b\n",
    "\n",
    "        gen_loss = total_loss_gen_a + total_loss_gen_b\n",
    "        dis_loss = gan_loss_dis_a + gan_loss_dis_b\n",
    "\n",
    "        if self.iteration % 3 == 0:\n",
    "            self.discriminator_a.cleargrads()\n",
    "            self.discriminator_b.cleargrads()\n",
    "            dis_loss.backward()\n",
    "            self._optimizers['discriminator_a'].update()\n",
    "            self._optimizers['discriminator_b'].update()\n",
    "        else:\n",
    "            self.generator_ab.cleargrads()\n",
    "            self.generator_ba.cleargrads()\n",
    "            gen_loss.backward()\n",
    "            self._optimizers['generator_ab'].update()\n",
    "            self._optimizers['generator_ba'].update()\n",
    "        \n",
    "        self.iteration = self.iteration+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#モデル生成\n",
    "gpu = 0 # 0：gpu使用、-1：gpu不使用\n",
    "generator_ab = Generator()\n",
    "generator_ba = Generator()\n",
    "discriminator_a = Discriminator()\n",
    "discriminator_b = Discriminator()\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    generator_ab.to_gpu()\n",
    "    generator_ba.to_gpu()\n",
    "    discriminator_a.to_gpu()\n",
    "    discriminator_b.to_gpu()\n",
    "    \n",
    "xp = cuda.cupy if gpu >= 0 else np\n",
    "\n",
    "opt_g_ab = chainer.optimizers.Adam(2e-4, beta1=0.5, beta2=0.999)\n",
    "opt_g_ab.setup(generator_ab)\n",
    "opt_g_ab.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "opt_g_ba = chainer.optimizers.Adam(2e-4, beta1=0.5, beta2=0.999)\n",
    "opt_g_ba.setup(generator_ba)\n",
    "opt_g_ba.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "\n",
    "opt_d_a = chainer.optimizers.Adam(2e-4, beta1=0.5, beta2=0.999)\n",
    "opt_d_a.setup(discriminator_a)\n",
    "opt_d_a.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "opt_d_b = chainer.optimizers.Adam(2e-4, beta1=0.5, beta2=0.999)\n",
    "opt_d_b.setup(discriminator_b)\n",
    "opt_d_b.add_hook(chainer.optimizer.WeightDecay(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#データ読み込み\n",
    "DataA = data.get_ori_data_x_1pic(fpath = 'imgA1.png')\n",
    "DataB = data.get_rand_core()#ランダム位置での球状のデータを作成\n",
    "\n",
    "updater = DiscoGANUpdater(DataA, DataB, opt_g_ab, opt_g_ba,opt_d_a, opt_d_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import gridplot, push_notebook, show, output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "ch, imgH, imgW = DataA['x'][0].shape\n",
    "\n",
    "img1 = data.get_view_img(DataA['x'][0])\n",
    "img2 = data.get_view_img(DataB['x'][0])\n",
    "\n",
    "plt1 = figure(title = 'train N = --', x_range=[0, imgW], y_range=[0, imgH])\n",
    "rend1 = plt1.image_rgba(image=[img1],x=[0], y=[0], dw=[imgW], dh=[imgH])\n",
    "\n",
    "plt2 = figure(title = 'count  = --', x_range=plt1.x_range, y_range=plt1.y_range)\n",
    "rend2 = plt2.image_rgba(image=[img2],x=[0], y=[0], dw=[imgW], dh=[imgH])\n",
    "\n",
    "plts = gridplot([[plt1,plt2]], plot_width=400, plot_height=400)\n",
    "handle = show(plts, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imgAトレーニングとテスト\n",
    "batchSize = 100\n",
    "N_train = 1000\n",
    "nLoop = 1000\n",
    "\n",
    "for loop in range(nLoop): \n",
    "    \n",
    "\n",
    "    #＜トレーニング＞\n",
    "    for i in range(0, N_train, batchSize):\n",
    "        \n",
    "        #アップデート\n",
    "        updater.update_core()\n",
    "        \n",
    "    # ＜テスト＞\n",
    "    x_a = chainer.Variable(xp.asarray(DataA['x']), volatile='on')\n",
    "    x_ab = generator_ab(x_a, test=False)\n",
    "    x_aba = generator_ba(x_ab, test=False)\n",
    "    \n",
    "    x_ab.to_cpu()\n",
    "    x_aba.to_cpu()\n",
    "    \n",
    "    DataA['ab']=x_ab.data\n",
    "    DataA['aba']=x_aba.data    \n",
    "    \n",
    "    #極大値の位置を抽出して、赤いサークルを元絵に重ねる\n",
    "    img_temp = DataA['ab'].sum(axis = 1)/3\n",
    "    DataA['ab_point'] = data.get_local_max_point(img_temp[:,np.newaxis,:,:],threshold = 50)\n",
    "    DataA['ab_circle']=data.draw_circle(DataA['ab_point'])\n",
    "    DataA['x_ab_circle'] = DataA['x'].copy()\n",
    "    DataA['x_ab_circle'][:,0,:,:]=DataA['x_ab_circle'][:,0,:,:] + DataA['ab_circle']\n",
    "    \n",
    "    #グラフィック表示    \n",
    "    img1 = data.get_view_img(DataA['x_ab_circle'][0])\n",
    "    img2 = data.get_view_img(DataA['ab'][0])\n",
    "    rend1.data_source.data['image'] = [img1]\n",
    "    rend2.data_source.data['image'] = [img2]\n",
    "    plt1.title.text='train N = '+ str((loop+1)*N_train)\n",
    "    plt2.title.text='count  =  '+ str(DataA['ab_point'].sum()/255)\n",
    "    push_notebook(handle = handle)#表示をアップデート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chainer.serializers.save_npz('generator_ab.npz',generator_ab)\n",
    "chainer.serializers.save_npz('generator_ba.npz',generator_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:oldenv]",
   "language": "python",
   "name": "conda-env-oldenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
